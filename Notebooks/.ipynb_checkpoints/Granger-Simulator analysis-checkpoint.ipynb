{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'simulate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-67e281242e4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../granger-busca\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimulate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrangerBusca\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhawkes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'simulate'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../granger-busca\")\n",
    "import numpy as np\n",
    "from gb import simulate, GrangerBusca\n",
    "import tick\n",
    "import tick.hawkes as hk\n",
    "import matplotlib.pyplot as plt\n",
    "from pp_metrics import *\n",
    "from time import time\n",
    "import scipy.stats as ss\n",
    "import os\n",
    "from random_search import *\n",
    "from gen_simulation import *\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_method_result_granger_nevents(model, n_events = 1000, num_simulations = 10):    \n",
    "    mu = model.mu_\n",
    "    Alpha = normalize(model.Alpha_.toarray(), \"l1\")\n",
    "    Beta = np.vstack((model.beta_,)*len(mu))\n",
    "    list_simulations = []\n",
    "    for i in range(num_simulations):\n",
    "        sim = simulate.GrangeBuscaSimulator(mu_rates = mu, Alpha_ba = Alpha, Beta_ba = np.vstack((model.beta_,)*len(mu)))\n",
    "        list_simulations.append(sim.simulate(n_events=n_events))\n",
    "    return list_simulations\n",
    "\n",
    "def simulate_method_result_granger(model, num_simulations = 10, simulation_time = 100):    \n",
    "    mu = model.mu_\n",
    "    Alpha = normalize(model.Alpha_.toarray(), \"l1\")\n",
    "    Beta = np.vstack((model.beta_,)*len(mu))\n",
    "    list_simulations = []\n",
    "    for i in range(num_simulations):\n",
    "        t1 = time()\n",
    "        sim = simulate.GrangeBuscaSimulator(mu_rates = mu, Alpha_ba = Alpha, Beta_ba = np.vstack((model.beta_,)*len(mu)))\n",
    "        list_simulations.append(sim.simulate(simulation_time))\n",
    "        print (i, time()-t1)\n",
    "    return list_simulations\n",
    "\n",
    "\n",
    "def readfile(filename):\n",
    "    if (\"memetracker\" not in filename):\n",
    "        tfilename = '../data/snap-baselines/'+file+'_ticks_top100.dat'\n",
    "        tgt = '../data/snap-baselines/'+file+'_groundtruth_top100.npy'\n",
    "    else:\n",
    "        tfilename = '../data/snap-baselines/'+file+'_ticks_top25.dat'\n",
    "        tgt = '../data/snap-baselines/'+file+'_groundtruth_top25.npy'\n",
    "\n",
    "    timestamps = []\n",
    "    with open(tfilename) as data:\n",
    "        for l in data:    \n",
    "            timestamps.append(np.array(sorted([float(x) for x in l.split()[1:]])))\n",
    "    groundTruth = np.array(np.load(tgt))\n",
    "\n",
    "    return (timestamps,groundTruth)\n",
    "\n",
    "def calc_delta(p1,p2,timestamps,t_idx):\n",
    "    tp=timestamps[p1][t_idx]\n",
    "    tpp_idx = bisect(timestamps[p2], tp)\n",
    "    if tpp_idx == len(timestamps[p2]):\n",
    "        tpp_idx -= 1\n",
    "    tpp = timestamps[p2][tpp_idx]\n",
    "    while tpp >= tp and tpp_idx > 0:\n",
    "        tpp_idx -= 1\n",
    "        tpp = timestamps[p2][tpp_idx]\n",
    "    if tpp >= tp:\n",
    "        return 0\n",
    "    return tp - tpp\n",
    "\n",
    "\n",
    "def calc_value(interval_len,delta_list,mu,Beta,Alpha,process_num):\n",
    "    value=mu[process_num]\n",
    "    for j in range(len(delta_list)):\n",
    "        if delta_list[j]>0:\n",
    "            value+=Alpha[j][process_num]/(Beta[j][process_num]+delta_list[j])\n",
    "    return (value*interval_len)\n",
    "\n",
    "def integral_granger(timestamps,proc_a,n_a,mu,Beta,Alpha):    \n",
    "    interval_len = timestamps[proc_a][n_a]-timestamps[proc_a][n_a-1]\n",
    "    delta_list=[]                \n",
    "    for proc_b in range(len(timestamps)):\n",
    "        if(proc_b==proc_a):\n",
    "            delta=timestamps[proc_a][n_a-1]-timestamps[proc_a][n_a-2]\n",
    "        else:\n",
    "            delta=calc_delta(proc_a, proc_b, timestamps, n_a-1)\n",
    "        if(delta==False):\n",
    "            delta_list.append(0)\n",
    "        else:\n",
    "            delta_list.append(delta)\n",
    "    if len(delta_list)==len(timestamps):\n",
    "        value=calc_value(interval_len,delta_list,mu,Beta,Alpha,process_num=proc_a)\n",
    "        return value\n",
    "    \n",
    "def get_zj_granger(timestamps,mu,Alpha,Beta):\n",
    "   \n",
    "    zjlist=[]\n",
    "    for proc_a in range(len(timestamps)): \n",
    "        for n_a in range(2,len(timestamps[proc_a])):\n",
    "            zjlist.append(integral_granger(timestamps,proc_a,n_a,mu,Beta,Alpha))\n",
    "            \n",
    "        #with Pool(10) as p:\n",
    "        #    zjlist += (p.starmap(integral_granger,args))\n",
    "\n",
    "    return zjlist\n",
    "\n",
    "def ks_plot(zjlist): \n",
    "    zjlist = np.array(zjlist)\n",
    "    n = len(zjlist)\n",
    "    zk = 1-np.exp(-zjlist) #transform into uniform\n",
    "    sorted_zk = sorted(zk)\n",
    "    y_values = np.array([(k-0.5)/n for k in range(1,n+1)])\n",
    "    plt.plot(sorted_zk, y_values, c='green')\n",
    "    plt.plot(y_values, y_values+1.36/(n**0.5), linestyle='--', c='k', alpha=0.75)\n",
    "    plt.plot(y_values, y_values-1.36/(n**0.5), linestyle='--', c='k', alpha=0.75)\n",
    "    plt.xlabel(\"Empirical CDF\")\n",
    "    plt.ylabel(\"Model CDF\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../granger-busca\")\n",
    "import numpy as np\n",
    "from gb import simulate, GrangerBusca\n",
    "import tick.hawkes as hk\n",
    "from pp_metrics import *\n",
    "import scipy.stats as ss\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "class GrangerExt(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, mtype = \"ADM4\", decay = None, C = None, lasso_nuclear_ratio = None, tol = None, kernel_support = None, kernel_size = None, integration_support = None, penalty = None, solver = None, step = None, beta = None):\n",
    "        self.mtype = mtype\n",
    "        self.error = False\n",
    "        if(self.mtype ==\"EM\"):\n",
    "            self.kernel_support = 10. if (kernel_support is None) else kernel_support\n",
    "            self.kernel_size = 10 if (kernel_size is None) else kernel_size\n",
    "            self.tol = 1e-05 if (tol is None) else tol\n",
    "        elif(self.mtype ==\"ADM4\"):\n",
    "            self.decay = 1. if (decay is None) else decay\n",
    "            self.C = 1000. if (C is None) else C\n",
    "            self.lasso_nuclear_ratio = 0.5 if (lasso_nuclear_ratio is None) else lasso_nuclear_ratio\n",
    "            self.tol = 1e-05 if (tol is None) else tol\n",
    "        elif(self.mtype ==\"HC\"):\n",
    "            self.integration_support = 20.0 if (integration_support is None) else integration_support\n",
    "            self.C = 1000. if (C is None) else C\n",
    "            self.penalty = 'none' if (penalty is None) else penalty\n",
    "            self.solver = 'adagrad' if (solver is None) else solver\n",
    "            self.step = 1e2 if (step is None) else step\n",
    "            self.tol = 1e-05 if (tol is None) else tol\n",
    "        elif(self.mtype ==\"Granger\"):\n",
    "            self.beta = 1 if (beta is None) else beta\n",
    "            \n",
    "    def fit(self, data):\n",
    "        timestamps = data[0]\n",
    "        if(self.mtype ==\"EM\"):\n",
    "            model = hk.HawkesEM(kernel_support = self.kernel_support, kernel_size = self.kernel_size, tol = self.tol, max_iter = 300, n_threads=8)\n",
    "            model.fit(timestamps)\n",
    "        elif(self.mtype ==\"ADM4\"):\n",
    "            model = hk.HawkesADM4(decay = self.decay, C = self.C, lasso_nuclear_ratio = self.lasso_nuclear_ratio, tol = self.tol, max_iter = 300, n_threads=8)\n",
    "            model.fit(timestamps)\n",
    "        elif(self.mtype ==\"HC\"):\n",
    "            try:\n",
    "                model = hk.HawkesCumulantMatching(integration_support = self.integration_support, C = self.C, penalty = self.penalty, solver = self.solver, step = self.step, tol = self.tol, max_iter = 300)\n",
    "                model.fit(timestamps)\n",
    "            except:\n",
    "                self.error = True\n",
    "        elif(self.mtype ==\"Granger\"):\n",
    "            model = GrangerBusca(alpha_prior = 1.0/len(timestamps), num_iter = 300, beta_strategy = self.beta)\n",
    "            model.fit(timestamps)\n",
    "        self.model = model\n",
    "\n",
    "    def score(self, data):\n",
    "        timestamps = data[0]\n",
    "        if(self.error == True):\n",
    "            return 1e-300\n",
    "        if(self.mtype ==\"EM\"):\n",
    "            return self.model.score()\n",
    "        elif(self.mtype ==\"ADM4\"):\n",
    "            return self.model.score()\n",
    "        elif(self.mtype ==\"HC\"):\n",
    "            return -self.model.objective(self.model.adjacency)\n",
    "        elif(self.mtype ==\"Granger\"):\n",
    "            g = granger_loglik(timestamps, self.model)\n",
    "            return g\n",
    "\n",
    "def select_hyper_EM(data, n_iter = 50):\n",
    "    g = GrangerExt(mtype = \"EM\")\n",
    "    param_dist = {\n",
    "        \"kernel_support\": ss.uniform(1, 150), \n",
    "        \"kernel_size\": ss.randint(1, 30), \n",
    "        \"tol\": ss.uniform(1e-6, 1e-4)\n",
    "    }\n",
    "    datafortrain = (data, data) #training is training and test\n",
    "    cv = ShuffleSplit(test_size = 1, n_splits = 1) \n",
    "    random_search = RandomizedSearchCV(g, param_distributions = param_dist, \n",
    "                                       n_iter = n_iter, cv = cv, verbose = 1, n_jobs = -1)\n",
    "    random_search.fit(datafortrain)\n",
    "    print(random_search.best_params_)\n",
    "    return (random_search.best_params_['kernel_support'], random_search.best_params_['kernel_size'], random_search.best_params_['tol'])\n",
    "\n",
    "def select_hyper_ADM4(data, n_iter = 50):\n",
    "    g = GrangerExt(mtype = \"ADM4\")\n",
    "    param_dist = {\n",
    "        \"decay\": ss.uniform(0, 100), \n",
    "        \"C\": ss.uniform(1e2, 1e4), \n",
    "        \"lasso_nuclear_ratio\": ss.uniform(0, 1), \n",
    "        \"tol\": ss.uniform(1e-6, 1e-4)\n",
    "    }\n",
    "    datafortrain = (data, data) #training is training and test\n",
    "    cv = ShuffleSplit(test_size = 1, n_splits = 1) \n",
    "    random_search = RandomizedSearchCV(g, param_distributions = param_dist, \n",
    "                                       n_iter = n_iter, cv = cv, verbose = 1, n_jobs = -1)\n",
    "    random_search.fit(datafortrain)\n",
    "    print(random_search.best_params_)\n",
    "    return (random_search.best_params_['decay'], random_search.best_params_['C'], random_search.best_params_['lasso_nuclear_ratio'], random_search.best_params_['tol'])\n",
    "\n",
    "def select_hyper_HC(data, n_iter = 50):\n",
    "    g = GrangerExt(mtype = \"HC\")\n",
    "    param_dist = {\n",
    "        \"integration_support\": ss.uniform(1, 100), \n",
    "        \"C\": ss.uniform(1e2, 1e4), \n",
    "        \"penalty\" : ['l1', 'l2', 'elasticnet', 'none'], \n",
    "        \"solver\" : [ 'adam', 'adagrad', 'rmsprop', 'adadelta'], \n",
    "        \"step\": ss.uniform(1e-1, 1e2), \n",
    "        \"tol\": ss.uniform(1e-6, 1e-4)\n",
    "    }\n",
    "    datafortrain = (data, data) #training is training and test\n",
    "    cv = ShuffleSplit(test_size = 1, n_splits = 1) \n",
    "    random_search = RandomizedSearchCV(g, param_distributions = param_dist, \n",
    "                                       n_iter = n_iter, verbose = 1, cv = cv, n_jobs = -1)\n",
    "    random_search.fit(datafortrain)\n",
    "    print(random_search.best_params_)\n",
    "    return (random_search.best_params_['integration_support'], random_search.best_params_['C'], random_search.best_params_['penalty'], \n",
    "            random_search.best_params_['solver'], random_search.best_params_['step'], random_search.best_params_['tol'])\n",
    "\n",
    "def select_hyper_Granger(data, n_iter = 150):\n",
    "    g = GrangerExt(mtype = \"Granger\")\n",
    "    param_dist = {\n",
    "        \"beta\": ss.uniform(1, 300000), \n",
    "    }\n",
    "    datafortrain = (data, data) #training is training and test\n",
    "    cv = ShuffleSplit(test_size = 1, n_splits = 1, random_state = 0) \n",
    "    random_search = RandomizedSearchCV(g, param_distributions = param_dist, \n",
    "                                       n_iter = n_iter, verbose = 1, cv = cv, n_jobs = -1)\n",
    "    random_search.fit(datafortrain)\n",
    "    print(random_search.best_params_)\n",
    "    return (random_search.best_params_['beta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 19.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beta': 239225.52954992696}\n",
      "beta 239225.52954992696\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 19.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beta': 149110.4285570589}\n",
      "beta 149110.4285570589\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 33.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beta': 61742.39306433025}\n",
      "beta 61742.39306433025\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 28.3min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 71.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beta': 28387.50888432489}\n",
      "beta 28387.50888432489\n",
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 41.7min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 105.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beta': 2338.68719742945}\n",
      "beta 2338.68719742945\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pylab as pl\n",
    "file_list = [\"sx-mathoverflow\", \"sx-superuser\", \"email-Eu-core-temporal\", \"wiki-talk-temporal\",\"memetracker_1month\"]\n",
    "\n",
    "\n",
    "np.seterr(over = 'raise', under = 'warn')\n",
    "percentTest = 0.25\n",
    "\n",
    "\n",
    "for file in file_list:\n",
    "    timestamps,groundTruth = readfile(file)\n",
    "    \n",
    "    minT = min([j for i in timestamps for j in i])\n",
    "    maxT = max([j for i in timestamps for j in i])\n",
    "    simTime = maxT-minT\n",
    "    sum_events = sum([len(i) for i in timestamps])\n",
    "    n_sims=1\n",
    "    \n",
    "    beta = select_hyper_Granger(timestamps, n_iter = 100)\n",
    "    print(\"beta\",beta)\n",
    "    #model = GrangerBusca(alpha_prior = 1.0/len(timestamps), num_iter = 300, beta_strategy = beta)\n",
    "    #model.fit(timestamps)\n",
    "    continue \n",
    "    sims = simulate_method_result_granger(model, simulation_time=simTime, num_simulations = n_sims)\n",
    "    #sims = simulate_method_result_granger_nevents(model, n_events = sum_events, num_simulations = n_sims)\n",
    "    counting = [np.arange(len(timestamps[i])) for i in range(len(timestamps))]\n",
    "    for i in range(len(timestamps)):\n",
    "        plt.plot(timestamps[i]-minT, counting[i], color='r')\n",
    "    \n",
    "    colors = pl.cm.tab20(np.linspace(0,1,n_sims+4))\n",
    "    j=0\n",
    "    avg=0\n",
    "    for sim in sims:\n",
    "        timestamps = sim\n",
    "        counting = [np.arange(len(timestamps[i])) for i in range(len(timestamps))]\n",
    "        for i in range(len(timestamps)):\n",
    "            plt.plot(timestamps[i], counting[i], color=colors[j], label='%d' % j)\n",
    "        j+=1\n",
    "        minT = min([j for i in sim for j in i])\n",
    "        maxT = max([j for i in sim for j in i])\n",
    "        simT = maxT\n",
    "        print (simTime/simT)\n",
    "        avg+=simTime/simT\n",
    "    #print(avg/n_sims)\n",
    "\n",
    "    break\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import PredefinedSplit\n",
    "(timestamps, groundTruth_granger, gtMu) = simulate_granger(n_points=30, d=3, alphaShape=\"alpha1\", n_clusters=5)\n",
    "train, test, groundTruth_granger = split_filter_train_test(timestamps, groundTruth_granger, percentTest, min_events=1)\n",
    "X = (train, train)\n",
    "test_fold = [-1, 0]\n",
    "ps = ShuffleSplit(test_size = 1, n_splits = 1) \n",
    "print(ps.get_n_splits(X))\n",
    "for train_index, test_index in ps.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-06151f0f188d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mAlpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAlpha_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"l1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;36m3\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mbinGT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroundTruth\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtrue_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinGT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "file = \"sx-askubuntu.txt.gz\"\n",
    "tfilename = '../data/snap-baselines/'+file[:-7]+'_ticks_top100.dat'\n",
    "tgt = '../data/snap-baselines/'+file[:-7]+'_groundtruth_top100.npy'\n",
    "timestamps = []\n",
    "with open(tfilename) as data:\n",
    "    for l in data:    \n",
    "        timestamps.append(np.array(sorted([float(x) for x in l.split()[1:]])))\n",
    "\n",
    "    minT = min([j for i in timestamps for j in i])\n",
    "\n",
    "groundTruth = np.array(np.load(tgt)).T\n",
    "\n",
    "beta = 1\n",
    "model = GrangerBusca(alpha_prior = 1.0/len(timestamps), num_iter = 300, beta_strategy = beta)\n",
    "model.fit(timestamps)\n",
    "Alpha = normalize(model.Alpha_.toarray(), \"l1\")\n",
    "\n",
    "binGT = groundTruth > 0\n",
    "true_values = binGT.flatten()\n",
    "y_values = Alpha.flatten()\n",
    "fpr, tpr, thresholds = metrics.roc_curve(true_values, y_values)\n",
    "plt.plot(fpr, tpr, alpha=0.8, lw = 1)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=1, color='r', alpha=.8)\n",
    "print(metrics.roc_auc_score(true_values, y_values))\n",
    "beta = 20000\n",
    "model = hk.HawkesADM4(decay = 5, n_threads=8)\n",
    "model.fit(timestamps)\n",
    "Alpha = model.get_kernel_norms()\n",
    "groundTruth = groundTruth.T\n",
    "binGT = groundTruth > 0\n",
    "true_values = binGT.flatten()\n",
    "y_values = Alpha.flatten()\n",
    "fpr, tpr, thresholds = metrics.roc_curve(true_values, y_values)\n",
    "plt.plot(fpr, tpr, color='g', alpha=0.8, lw = 1)\n",
    "print(metrics.roc_auc_score(true_values, y_values))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbb57234b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "y_true = []\n",
    "y = []\n",
    "\n",
    "binGT = groundTruth > 0.\n",
    "for i in range(len(Alpha)):\n",
    "    for j in range(len(Alpha)):\n",
    "        y_true.append(binGT[i][j])\n",
    "        y.append(Alpha[i][j])\n",
    "\n",
    "plt.show()\n",
    "plt.hist(y, bins=np.arange(0,1,0.001))\n",
    "plt.xlim([0.005,0.2])\n",
    "plt.ylim([0.0,10])\n",
    "Alpha\n",
    "print(sum(Alpha[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.9673461103387317\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generalized_distance(A_true, A_pred, swap_cost = lambda pos:1, dist_metric = lambda x,y: 1):\n",
    "    distlist = []\n",
    "    for i in range(len(A_pred)):\n",
    "        v1 = A_true[i]\n",
    "        v2 = A_pred[i]\n",
    "        rank1 = [int(i) for i in ss.rankdata(v1)]\n",
    "        rank2 = [int(i) for i in ss.rankdata(v2)]\n",
    "        dist = 0\n",
    "        for idx1 in range(len(v1)):\n",
    "            for idx2 in range(idx1, len(v1)):\n",
    "                if (v1[idx1]<v1[idx2] and v2[idx1]>v2[idx2]) or (v1[idx1]>v1[idx2] and v2[idx1]<v2[idx2]):\n",
    "                    pi1 = sum([swap_cost(i) for i in range(1,rank1[idx1])])\n",
    "                    pi2 = sum([swap_cost(i) for i in range(1,rank1[idx2])])\n",
    "                    pi_bar = (pi1-pi2)/(rank1[idx1]-rank1[idx2])\n",
    "\n",
    "                    pj1 = sum([swap_cost(i) for i in range(1,rank2[idx1])])\n",
    "                    pj2 = sum([swap_cost(i) for i in range(1,rank2[idx2])])\n",
    "                    pj_bar = (pj1-pj2)/(rank2[idx1]-rank2[idx2])\n",
    "                    dist += v1[idx1]*v2[idx2]*pi_bar*pj_bar*dist_metric(rank2[idx1],rank2[idx2])\n",
    "        distlist.append(dist)\n",
    "    return (np.mean(distlist), distlist)\n",
    "\n",
    "def NRMSE(A_true, A_pred):\n",
    "    err = []\n",
    "    for i in range(len(A_true)):\n",
    "        v = 0\n",
    "        for j in range(len(A_true[i])):\n",
    "            v += (A_true[i][j]-A_pred[i][j])**2\n",
    "        v = ((1/len(A_true[i]))*v)**0.5/(max(A_true[i])-min(A_true[i]))\n",
    "        err.append(v)\n",
    "    return (np.mean(err),err)\n",
    "\n",
    "\n",
    "def MAP(A_true, A_pred):\n",
    "    map_avg = []\n",
    "    binGT = A_true > 0.\n",
    "    for i in range(len(A_pred)):\n",
    "        map = 0\n",
    "        sorted_idx = sorted(range(len(A_pred[i])), key=lambda x:A_pred[i][x], reverse=True)\n",
    "        tp = 0\n",
    "        fn = len(A_pred[i])\n",
    "        fp = 0\n",
    "        MAP = 0\n",
    "        for j in sorted_idx:\n",
    "            if binGT[i][j] == True:\n",
    "                tp += 1\n",
    "                MAP += tp/(tp+fp)*(1/len(A_pred[i]))\n",
    "            else:\n",
    "                fp += 1\n",
    "            fn -= 1\n",
    "        map_avg.append(MAP)  \n",
    "    return (np.mean(map_avg), map_avg)\n",
    "\n",
    "\n",
    "groundTruth = np.array([[2,5],[2,4]])\n",
    "Alpha = [[8,8],[3,4]]\n",
    "print(generalized_distance(groundTruth,Alpha)[0])\n",
    "print(NRMSE(groundTruth,Alpha)[0])\n",
    "print(MAP(groundTruth,Alpha)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CollegeMsg.txt.gz\n",
      "1.0\n",
      "sx-askubuntu.txt.gz\n",
      "1.0\n",
      "email-Eu-core-temporal.txt.gz\n",
      "1.0\n",
      "sx-mathoverflow.txt.gz\n",
      "1.0\n",
      "soc-sign-bitcoinalpha.csv.gz\n",
      "1.0\n",
      "sx-superuser.txt.gz\n",
      "1.0\n",
      "soc-sign-bitcoinotc.csv.gz\n",
      "1.0\n",
      "wiki-talk-temporal.txt.gz\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import gzip\n",
    "from scipy import sparse as sp\n",
    "\n",
    "def get_graph_stamps(path, top = None):\n",
    "    count = defaultdict(int)\n",
    "    srcs = set()\n",
    "    with gzip.open(path, 'r') as in_file:\n",
    "        for line in in_file:\n",
    "            if b',' in line:\n",
    "                spl = line.split(b',')\n",
    "            else:\n",
    "                spl = line.split()\n",
    "            src, dst = spl[:2]\n",
    "            count[dst] += 1\n",
    "            srcs.add(src)\n",
    "\n",
    "    if top is None:\n",
    "        valid = srcs\n",
    "    else:\n",
    "        valid = set()\n",
    "        for v, k in sorted(((v, k) for k, v in count.items()), reverse=True):\n",
    "            if k in srcs:\n",
    "                valid.add(k)\n",
    "                if len(valid) == top:\n",
    "                    break\n",
    "\n",
    "    graph = {}\n",
    "    ids = {}\n",
    "    with gzip.open(path, 'r') as in_file:\n",
    "        timestamps = []\n",
    "        for line in in_file:\n",
    "            if b',' in line:\n",
    "                spl = line.split(b',')\n",
    "            else:\n",
    "                spl = line.split()\n",
    "            src, dst = spl[:2]\n",
    "            stamp = float(spl[-1])\n",
    "            if src not in valid:\n",
    "                continue\n",
    "            if dst not in valid:\n",
    "                continue\n",
    "\n",
    "            if src not in graph:\n",
    "                graph[src] = {}\n",
    "            if dst not in graph[src]:\n",
    "                graph[src][dst] = 0\n",
    "            graph[src][dst] += 1\n",
    "\n",
    "            if dst in ids:\n",
    "                timestamps[ids[dst]].append(stamp)\n",
    "            else:\n",
    "                ids[dst] = len(timestamps)\n",
    "                timestamps.append([stamp])\n",
    "\n",
    "    for id_ in list(graph.keys()):\n",
    "        if id_ not in ids:\n",
    "            del graph[id_]\n",
    "    for id_ in ids:\n",
    "        if id_ not in graph:\n",
    "            graph[id_] = {}\n",
    "\n",
    "    return timestamps, graph, ids\n",
    "\n",
    "\n",
    "file_list=[\"CollegeMsg.txt.gz\", \"sx-askubuntu.txt.gz\", \"email-Eu-core-temporal.txt.gz\", \"sx-mathoverflow.txt.gz\", \"soc-sign-bitcoinalpha.csv.gz\", \"sx-superuser.txt.gz\", \"soc-sign-bitcoinotc.csv.gz\", \"wiki-talk-temporal.txt.gz\"]\n",
    "\n",
    "for file in file_list :\n",
    "    print(file)\n",
    "    timestamps, graph,ids = get_graph_stamps(file,top = 100)\n",
    "\n",
    "    gt_matrix = np.zeros(shape = (len(ids),(len(ids))))\n",
    "    sorted_ids = sorted(ids.keys(),key = lambda x:ids[x])\n",
    "    for src in sorted_ids:\n",
    "        for dst in sorted_ids:\n",
    "            if(dst in graph[src]):\n",
    "                gt_matrix[ids[src]][ids[dst]] = graph[src][dst]\n",
    "    #gt_matrix = normalize(gt_matrix,'l1', axis = 0)\n",
    "    vals = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "    for src in graph:\n",
    "        for dst in graph[src]:\n",
    "            assert src in ids and dst in ids\n",
    "            rows.append(ids[src])\n",
    "            cols.append(ids[dst])\n",
    "            vals.append(graph[src][dst])\n",
    "    GT = sp.csr_matrix((vals, (rows, cols)), dtype='d').toarray()\n",
    "    #GT = normalize(GT, 'l1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
